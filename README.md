## TCMBench: Benchmarking Large Language Models in Traditional Chinese Medicine from Knowledge to Clinical Reasoning
Repo for TCMBench (â€œShuzhiQihuangâ€ LLMs seriesï¼ŒThe first comprehensive benchmark for evaluating LLMs in TCM)

[**English**](./README.md) | [**ä¸­æ–‡**](./README_Chinese.md)

<p align="center">
    <br>
    <img src="./image/TCMBench_logo.png" width="355"/>
    <br>
</p>
<p align="center">
    <img alt="GitHub" src="https://img.shields.io/github/license/ymcui/Chinese-LLaMA-Alpaca.svg?color=blue&style=flat-square">
    <img alt="GitHub top language" src="https://img.shields.io/github/languages/top/ymcui/Chinese-LLaMA-Alpaca">
</p>

## Updates

ğŸ’¥ TCMBench V2.0 is here! In this version, we added test questions that reflect the multi-standard and multi-factor characteristics of dynamic clinical reasoning in TCM. We also generated new questions with reasoning perturbations, forming three levels of evaluation tasks and 13 sub-tasks in total.

ğŸš€ The initial version of the paper has been released. Citations are welcome. â— All forms of plagiarism are strictly rejected (smile.jpg).

## âš¡ Introduction

To further evaluate the performance of large language models (LLMs) in Traditional Chinese Medicine (TCM) more effectively and accurately, we established a standardized and comprehensive benchmark framework: TCMBench. This benchmark fully considers the complexity and domain-specific nature of TCM, covering multiple aspects to ensure the practical usability and applicability of LLMs in real-world TCM scenarios.

ğŸ“š Dataset: TCMEval

We first constructed the TCMEval dataset, the first benchmark dataset in TCM. To objectively and accurately reflect the knowledge system and clinical reasoning characteristics of TCM, TCMEval is built using high-quality simulated questions from the TCM Licensing Examination as the data source.

The dataset contains 6,482 questionâ€“answer pairs, of which 1,300 pairs are accompanied by official standard explanations for evaluating the generation quality of LLMs. All data avoids personal information and focuses on TCM knowledge and clinical content.

Under the guidance of TCM experts, the original questions were filtered and confirmed. From each subject and question type, no more than 100 samples were randomly selected while ensuring an even distribution of answer options to avoid data bias. Two graduate students in TCM further verified the questions, ensuring full coverage of all exam subjects and question types. Through this collection, organization, and annotation process, TCMEval provides a comprehensive, accurate, and representative TCM benchmark to support the evaluation and improvement of LLM applications in TCM.

**ğŸ” Task Types**:
- ğŸš€ **Fundamental Knowledge Cognition Tasks**:The lowest complexity level includes 5,473 Q&A pairs. This task set is based on standard question types in the TCMLE exam and subdivided into three representative tasks, each reflecting different dimensions of knowledge cognition:
	- Fundamental Knowledge Understanding (FKU)
	- Knowledge Horizontal Correlation (KHC)
	- Clinical Vertical Reasoning (CVR)
    Data available at [./data/first_level](./data/first_level).
- ğŸš€ **Comprehensive Dynamic Clinical Analysis Tasks**:Built on top of the fundamental knowledge cognition tasks and designed with input from TCM experts, this set includes six types of tasks featuring multi-standard diagnosis and treatment (e.g., syndrome differentiation and treatment, one disease with different treatments, different diseases with the same treatment) and multi-factor reasoning (e.g., social environment, classical literature interpretation, and philosophical understanding).
This set contains 883 Q&A pairs, with at least 50 samples per task to ensure stable evaluation.
Data available at [./data/second_level](./data/second_level)l.
- ğŸš€ **Complex Clinical Decision-Making Tasks**:To further evaluate the comprehensive reasoning ability and stability of LLMs in multi-standard TCM clinical environments, we designed four types of complex decision-making tasks based on the dynamic clinical analysis tasks. By restructuring original reasoning samples, introducing semantic perturbations, and converting task formats (e.g., perturbations in syndrome differentiation and treatment, one disease with different treatments, different diseases with the same treatment, as well as Chinese medicine prescription tasks), we generated 1,009 new Q&A pairs.This task set systematically examines model consistency in high-complexity reasoning, robustness in inference, and stability in decision-making.
Data available at [./data/third_level](./data/third_level).

## ğŸ‘¨â€âš•ï¸ Data Processing

### Example: Fundamental Knowledge Cognition Tasks

We converted the test questions into structured evaluation data. The data format is as follows:

Fundamental Knowledge Understanding Task
```json
 {
      "question": "ã€Šç´ é—®Â·å’³è®ºã€‹ï¼šâ€œäº”è„å…­è…‘çš†ä»¤äººå’³â€ï¼Œä½†å…³ç³»æœ€å¯†åˆ‡çš„æ˜¯ï¼ˆ  ï¼‰ã€‚\nAï¼å¿ƒè‚º\nBï¼è‚ºè‚¾\nCï¼è‚ºè„¾\nDï¼è‚ºèƒƒ\nEï¼è‚ºå¤§è‚ ",
      "answer": [
        "D"
      ],
      "analysis": "æ ¹æ®ã€Šç´ é—®Â·å’³è®ºã€‹â€œæ­¤çš†èšäºèƒƒï¼Œå…³äºè‚ºï¼Œä½¿äººå¤šæ¶•å”¾è€Œé¢æµ®è‚¿æ°”é€†ä¹Ÿâ€å¯çŸ¥ä¸äº”è„å…­è…‘çš†ä»¤äººå’³å…³ç³»æœ€å¯†åˆ‡çš„è„è…‘ä¸ºè‚ºèƒƒã€‚æ‰‹å¤ªé˜´è‚ºç»èµ·äºä¸­ç„¦ï¼Œè¿˜å¾ªèƒƒå£ï¼Œä¸Šè†ˆå±è‚ºã€‚å¯’å‡‰é¥®é£Ÿå…¥èƒƒï¼Œå¯¼è‡´ä¸­ç„¦å¯’ï¼Œå¯’æ°”å¾ªæ‰‹å¤ªé˜´è‚ºç»ä¸Šå…¥äºè‚ºä¸­ï¼Œå¯¼è‡´è‚ºå¯’ï¼Œè‚ºä¸ºå¨‡è„ï¼Œä¸è€å¯’çƒ­ï¼Œå¤–å†…å¯’é‚ªå¹¶èšäºè‚ºï¼Œåˆ™è‚ºå¤±å®£é™ï¼Œè‚ºæ°”ä¸Šé€†å‘ç”Ÿå’³å—½ã€‚å› æ­¤ç­”æ¡ˆé€‰Dã€‚",
      "knowledge_point": "ä¸­åŒ»ç»å…¸",
      "index": 8196,
      "score": 1
    }
```
Clinical Vertical Reasoning Taskï¼š
```json
    {
      "share_content": "åˆ˜Ã—ï¼Œç”·ï¼Œ46å²ï¼Œåˆ»ä¸‹çœ©æ™•è€Œè§å¤´é‡å¦‚è’™ã€‚èƒ¸é—·æ¶å¿ƒï¼Œé£Ÿå°‘å¤šå¯ï¼Œè‹”ç™½è…»ï¼Œè„‰æ¿¡æ»‘ã€‚",
      "question": [
        {
          "sub_question": "1)ï¼è¯å±ï¼ˆ  ï¼‰ã€‚\nAï¼è‚é˜³ä¸Šäº¢\nBï¼æ°”è¡€äºè™š\nCï¼è‚¾ç²¾ä¸è¶³\nDï¼ç—°æµŠä¸­é˜»\nEï¼ä»¥ä¸Šéƒ½ä¸æ˜¯\n",
          "answer": [
            "D"
          ],
          "analysis": ""
        },
        {
          "sub_question": "2)ï¼æ²»æ³•å®œé€‰ï¼ˆ  ï¼‰ã€‚\nAï¼ç‡¥æ¹¿ç¥›ç—°ï¼Œå¥è„¾å’Œèƒƒ\nBï¼è¡¥è‚¾æ»‹é˜´\nCï¼è¡¥è‚¾åŠ©é˜³\nDï¼è¡¥å…»æ°”è¡€ï¼Œå¥è¿è„¾èƒƒ\nEï¼å¹³è‚æ½œé˜³ï¼Œæ»‹å…»è‚è‚¾\n",
          "answer": [
            "A"
          ],
          "analysis": ""
        },
        {
          "sub_question": "3)ï¼æ–¹è¯å®œé€‰ï¼ˆ  ï¼‰ã€‚\nAï¼å³å½’ä¸¸\nBï¼å·¦å½’ä¸¸\nCï¼åŠå¤ç™½æœ¯å¤©éº»æ±¤\nDï¼å½’è„¾æ±¤\nEï¼å¤©éº»é’©è—¤é¥®\n",
          "answer": [
            "C"
          ],
          "analysis": ""
        }
      ],
      "knowledge_point": "ä¸­åŒ»å†…ç§‘å­¦",
      "index": 334,
      "score": 1
    }
```
Knowledge Horizontal Correlation Taskï¼š
```json
  {
      "share_content": "ï¼ˆå…±ç”¨å¤‡é€‰ç­”æ¡ˆï¼‰\nA.åŒ–ç—°æ¯é£ï¼Œå¥è„¾ç¥›æ¹¿\nB.æ¸…è‚ºåŒ–ç—°ï¼Œæ•£ç»“æ’è„“\nC.ç–é£å®£è‚ºï¼ŒåŒ–ç—°æ­¢å’³\nD.æ¸…çƒ­åŒ–ç—°ï¼Œå¹³è‚æ¯é£\nE.æ¶¦è‚ºæ¸…çƒ­ï¼Œç†æ°”åŒ–ç—°\n",
      "question": [
        {
          "sub_question": "1)ï¼è´æ¯ç“œè’Œæ•£çš„åŠŸç”¨æ˜¯ï¼ˆ  ï¼‰ã€‚",
          "answer": [
            "E"
          ],
          "analysis": ""
        },
        {
          "sub_question": "2)ï¼åŠå¤ç™½æœ¯å¤©éº»æ±¤çš„åŠŸç”¨æ˜¯ï¼ˆ  ï¼‰ã€‚",
          "answer": [
            "A"
          ],
          "analysis": ""
        }
      ],
      "knowledge_point": "æ–¹å‰‚å­¦",
      "index": 1938,
      "score": 1
    }
```

## ğŸ§ Evaluation Details

We designed task-adaptive prompts that require LLMs to answer questions and provide explanations. The evaluation framework consists of the following components:

| æ–‡ä»¶å                     | è¯´æ˜           |
| -------------------------- | -------------- |
| [./pipline/choice_bench.py](./pipline/choice_bench.py)     | Set up different tasks and guide LLMs to generate answers and explanations|
| [./pipline/bench_function.py](./pipline/bench_function.py)   | Functions for testing   |
| [./pipline/Acc.py](./pipline/Acc.py) | Compute accuracy   |
| [./pipline/Model_API.py](./pipline/Model_API.py)| Call model APIs (OpenAI as an example), adjustable for different models |
| [./TCMBench_code/explain_evaluation.py](./TCMBench_code/explain_evaluation.py)| Evaluate explanation quality using ROUGE-1, ROUGE-L, SARI, BERTScore, BartScore, and our proposed SKScore |
|[./HumanTrue.json](./HumanTrue.json)| HumanTrue Dataset|


First, run [/pipline/choice_bench.py](./pipline/choice_bench.py) to test models and obtain their generated answers and explanations:
```
# (Optional) If needed, set your proxy:
os.environ['HTTPS_PROXY']="your proxy"

# If using closed-source models, enter your API key; otherwise leave blank:
api_key = "your key"

# Specify --data_path and --sys_prompt for different tasks, and --model_name to call different models.
# Example: run the FKU task on gpt-4-0613
python choice_bench.py --data_path ../data/first_level --sys_prompt FKU.json --model_name gpt-4-0613
```

Use [./pipline/Acc.py](./pipline/Acc.py) to compute accuracy scores for different models on different tasks by setting --data_path, --queation_type, and --model_name:
 ```
python Acc.py --data_path ../data/first_level --queation_type FKU --model_name gpt-4-0613
 ```

Use [./TCMBench_code/explain_evaluation.py](./TCMBench_code/explain_evaluation.py) to compute explanation scores across six metrics by specifying --model_name:
 ```
python explain_evaluation.py --model_name gpt-4-0613
 ```
The models used for these metrics can be found at [code link](https://huggingface.co/WJing123/TCMBench_code)


ğŸ‘¨â€âš•ï¸ In addition, this work also introduces our previously developed TCM LLMs, ShenNong. We welcome everyone to follow our open-source TCM large language model project ShenNong-TCM-LLM: **ShenNong-TCM-LLM**ï¼š

- ğŸš€ [ShenNong-TCM](https://github.com/ywjawmw/ShenNong-TCM-LLM) : To promote the development and real-world application of LLMs in Traditional Chinese Medicine, we released ShenNong, a large-scale TCM language model designed to improve knowledge coverage and enhance its ability to answer medical consultations in TCM. It is built upon the [TCM instruction dataset SN-QA](https://huggingface.co/datasets/michaelwzhu/ShenNong_TCM_Dataset)ã€‚

We also introduce our other open-source healthcare LLM projects:
- ğŸš€ [Intelligent TCM Inheritance and Innovation Platform](https://github.com/ywjawmw/AI4TCM-Platform) : As part of the Shuzhi Qihuang series, this platform addresses two main challenges: 1.	The inability of existing TCM platforms to cover multimodal data by constructing a more comprehensive knowledge graph integrating TCM and Western medicine. 2.	The inefficiency in TCM experience inheritance by proposing interpretable prescription analysis technology that automatically analyzes the holistic diagnostic process from symptoms to prescriptions and provides scientific reasoning. It also provides a fair platform to help young doctors and TCM students quickly master advanced knowledge and inherit medical expertise.
- ğŸš€ [ChatMed-Consult](https://huggingface.co/michaelwzhu/ChatMed-Consult) : Built from the [ChatMed_Consult_Dataset](https://huggingface.co/datasets/michaelwzhu/ChatMed_Consult_Dataset) with over 500k online medical consultations paired with ChatGPT responses. The model backbone is [LlaMA-7b](https://github.com/facebookresearch/llama), combined with LoRA weights from [Chinese-LlaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca) and extended Chinese vocabulary, followed by efficient parameter tuning using LoRA. All codes are publicly released.
  
- ğŸš€ [PromptCBLUEä¸­æ–‡åŒ»ç–—å¤§æ¨¡å‹è¯„æµ‹åŸºå‡†](https://github.com/michael-wzhu/PromptCBLUE): A prompt-based adaptation of the [CBLUE](https://tianchi.aliyun.com/dataset/95414) benchmark, designed to evaluate Chinese medical knowledge and text-processing abilities of LLMs. PromptCBLUE enables a single generative LLM to handle a variety of medical NLP tasks such as medical record structuring, consultation, and clinical documentation writing.

## Acknowledgements

This project was developed based on APIs of large language models and inspired by evaluation tasks on Gaokao examination questions. We thank the related projects and developers for their contributions:

- [ChatGPT](https://openai.com/blog/chatgpt)
- [ChatGLM](https://github.com/THUDM/ChatGLM-6B)
- [GaoKao-Bench](https://github.com/OpenLMLab/GAOKAO-Bench)


## Citation

If you use the data or code from this project, please cite:

```bash
@misc{yue2023 TCMBench,
      title={TCMBench: Benchmarking Large Language Models in Traditional Chinese Medicine from Knowledge to Clinical Reasoning}, 
      author={Wenjing Yue, Ming guan, Wei Zhu, Xiaoling Wang , Saisai Tian and Weidong Zhang},
      year={2023},
      publisher = {GitHub},
      journal = {GitHub repository},
      howpublished = {\url{https://github.com/ywjawmw/TCMBench}},
}

```

## Team

This project was completed under the guidance of **Prof. Xiaoling Wang** from the School of Computer Science and Technology, East China Normal University.





