# TCMBench: A Comprehensive Benchmark for Evaluating Large Language Models in Traditional Chinese Medicine
Repo for TCMBench (The first comprehensive benchmark for evaluating LLMs in TCM)

The [paper](https://arxiv.org/abs/2406.01126) has been submitted to XXX.

[**English**](./README.md) | [**ä¸­æ–‡**](./README_Chinese.md)

<p align="center">
    <img alt="GitHub" src="https://img.shields.io/github/license/ymcui/Chinese-LLaMA-Alpaca.svg?color=blue&style=flat-square">
    <img alt="GitHub top language" src="https://img.shields.io/github/languages/top/ymcui/Chinese-LLaMA-Alpaca">
</p>

## âš¡ ç®€ä»‹
Large Language Models (LLMs) excel in various natural language processing tasks but lack dedicated benchmarks for traditional Chinese medicine (TCM). To fill this gap, we introduce **TCMBench**, a comprehensive benchmark for evaluating LLMs in TCM. 


## ğŸ“š Datasetï¼šTCM-ED
The TCMLE assesses whether applicants possess the necessary professional knowledge and skills to practice as TCM physicians. Therefore, we collect 5,473 representative practice questions. Among them, the data we collect does not contain personal information but focuses on selecting data instances that can fully reflect and represent theoretical knowledge and practical skills in TCM. The multiple-choice questions in TCMLE. are divided into three categories: A1/A2, A3, and B1. A1/A2 questions consist of one question with five options. A3 questions include multiple sub-questions that share a typical clinical case, simulating actual clinical scenarios. B1 questions also contain multiple sub-questions but share the same five options. Consequently, A3 and B1 questions establish a logical correlation in clinical and knowledge, respectively. For detailed descriptions and examples of three types of questions, please refer to section A of appendix files.

**ğŸ” é¢˜ç›®ç±»å‹** ï¼š
- ğŸš€ **The single-sentence best-choice questions(A1) and the case summary best-choice questions(A2) type**: It consists of a question stem and five options with correct one.
- ğŸš€ **The best choice questions for case group(A3) type**: The stem presents a patient-centered case, followed by multiple sub-questions, each offering five options with one correct answer. It primarily centers on clinical applications.
- ğŸš€ **The standard compatibility questions(B1) type**: Multiple sub-questions share the same five options, where each option may be chosen zero, one, or multiple times. There is one correct answer among the five options for each sub-question.


TCM-EDï¼š

| Question type           | A1/A2 type| A3 type|B1 type|
| ------------------ | -------------- | -------------- |-------------- |
| **Number of questions**           | 1600           | 198         |1481         |
| **Number of sub-questions**             | \           | 642          | 3231          |



## ğŸ‘¨â€âš•ï¸ Data Processing

A1/A2 typeï¼š
```json
 {
      "question": "ã€Šç´ é—®Â·å’³è®ºã€‹ï¼šâ€œäº”è„å…­è…‘çš†ä»¤äººå’³â€ï¼Œä½†å…³ç³»æœ€å¯†åˆ‡çš„æ˜¯ï¼ˆ  ï¼‰ã€‚\nAï¼å¿ƒè‚º\nBï¼è‚ºè‚¾\nCï¼è‚ºè„¾\nDï¼è‚ºèƒƒ\nEï¼è‚ºå¤§è‚ ",
      "answer": [
        "D"
      ],
      "analysis": "æ ¹æ®ã€Šç´ é—®Â·å’³è®ºã€‹â€œæ­¤çš†èšäºèƒƒï¼Œå…³äºè‚ºï¼Œä½¿äººå¤šæ¶•å”¾è€Œé¢æµ®è‚¿æ°”é€†ä¹Ÿâ€å¯çŸ¥ä¸äº”è„å…­è…‘çš†ä»¤äººå’³å…³ç³»æœ€å¯†åˆ‡çš„è„è…‘ä¸ºè‚ºèƒƒã€‚æ‰‹å¤ªé˜´è‚ºç»èµ·äºä¸­ç„¦ï¼Œè¿˜å¾ªèƒƒå£ï¼Œä¸Šè†ˆå±è‚ºã€‚å¯’å‡‰é¥®é£Ÿå…¥èƒƒï¼Œå¯¼è‡´ä¸­ç„¦å¯’ï¼Œå¯’æ°”å¾ªæ‰‹å¤ªé˜´è‚ºç»ä¸Šå…¥äºè‚ºä¸­ï¼Œå¯¼è‡´è‚ºå¯’ï¼Œè‚ºä¸ºå¨‡è„ï¼Œä¸è€å¯’çƒ­ï¼Œå¤–å†…å¯’é‚ªå¹¶èšäºè‚ºï¼Œåˆ™è‚ºå¤±å®£é™ï¼Œè‚ºæ°”ä¸Šé€†å‘ç”Ÿå’³å—½ã€‚å› æ­¤ç­”æ¡ˆé€‰Dã€‚",
      "knowledge_point": "ä¸­åŒ»ç»å…¸",
      "index": 8196,
      "score": 1
    }
```
A3 typeï¼š
```json
    {
      "share_content": "åˆ˜Ã—ï¼Œç”·ï¼Œ46å²ï¼Œåˆ»ä¸‹çœ©æ™•è€Œè§å¤´é‡å¦‚è’™ã€‚èƒ¸é—·æ¶å¿ƒï¼Œé£Ÿå°‘å¤šå¯ï¼Œè‹”ç™½è…»ï¼Œè„‰æ¿¡æ»‘ã€‚",
      "question": [
        {
          "sub_question": "1)ï¼è¯å±ï¼ˆ  ï¼‰ã€‚\nAï¼è‚é˜³ä¸Šäº¢\nBï¼æ°”è¡€äºè™š\nCï¼è‚¾ç²¾ä¸è¶³\nDï¼ç—°æµŠä¸­é˜»\nEï¼ä»¥ä¸Šéƒ½ä¸æ˜¯\n",
          "answer": [
            "D"
          ],
          "analysis": ""
        },
        {
          "sub_question": "2)ï¼æ²»æ³•å®œé€‰ï¼ˆ  ï¼‰ã€‚\nAï¼ç‡¥æ¹¿ç¥›ç—°ï¼Œå¥è„¾å’Œèƒƒ\nBï¼è¡¥è‚¾æ»‹é˜´\nCï¼è¡¥è‚¾åŠ©é˜³\nDï¼è¡¥å…»æ°”è¡€ï¼Œå¥è¿è„¾èƒƒ\nEï¼å¹³è‚æ½œé˜³ï¼Œæ»‹å…»è‚è‚¾\n",
          "answer": [
            "A"
          ],
          "analysis": ""
        },
        {
          "sub_question": "3)ï¼æ–¹è¯å®œé€‰ï¼ˆ  ï¼‰ã€‚\nAï¼å³å½’ä¸¸\nBï¼å·¦å½’ä¸¸\nCï¼åŠå¤ç™½æœ¯å¤©éº»æ±¤\nDï¼å½’è„¾æ±¤\nEï¼å¤©éº»é’©è—¤é¥®\n",
          "answer": [
            "C"
          ],
          "analysis": ""
        }
      ],
      "knowledge_point": "ä¸­åŒ»å†…ç§‘å­¦",
      "index": 334,
      "score": 1
    }
```
B1 typeï¼š
```json
  {
      "share_content": "ï¼ˆå…±ç”¨å¤‡é€‰ç­”æ¡ˆï¼‰\nA.åŒ–ç—°æ¯é£ï¼Œå¥è„¾ç¥›æ¹¿\nB.æ¸…è‚ºåŒ–ç—°ï¼Œæ•£ç»“æ’è„“\nC.ç–é£å®£è‚ºï¼ŒåŒ–ç—°æ­¢å’³\nD.æ¸…çƒ­åŒ–ç—°ï¼Œå¹³è‚æ¯é£\nE.æ¶¦è‚ºæ¸…çƒ­ï¼Œç†æ°”åŒ–ç—°\n",
      "question": [
        {
          "sub_question": "1)ï¼è´æ¯ç“œè’Œæ•£çš„åŠŸç”¨æ˜¯ï¼ˆ  ï¼‰ã€‚",
          "answer": [
            "E"
          ],
          "analysis": ""
        },
        {
          "sub_question": "2)ï¼åŠå¤ç™½æœ¯å¤©éº»æ±¤çš„åŠŸç”¨æ˜¯ï¼ˆ  ï¼‰ã€‚",
          "answer": [
            "A"
          ],
          "analysis": ""
        }
      ],
      "knowledge_point": "æ–¹å‰‚å­¦",
      "index": 1938,
      "score": 1
    }
```

## ğŸ§ Evaluation Pipeline


| File Name                     | Description           |
| -------------------------- | -------------- |
| /evaluate/A12_bench.py     | Generating answers for A1/A2 type of quesions |
| /evaluate/A3-B1_bench.py      | Generating answers for A3/B1 type of quesions|
| /evaluate/bench_function.py   | Test   |
| /evaluate/correct_analyse.py  | Accuracy Metric   |
| /prompt/A1-2_prompt.json| Prompt of A1/A2 type of quesions| 
| /prompt/A3-4_prompt.json| Prompt of A3 type of quesions| 
| /prompt/B1_prompt.json| Prompt of B1 type of quesions| 
| /models/Openai.py| Model API(eg. openai) |




You can run with API[A12_bench.py](https://github.com/ywjawmw/ShenNong-TCM-Evaluation-BenchMark/blob/main/evaluate/A12_bench.py)/[A3-B1_bench.py](https://github.com/ywjawmw/ShenNong-TCM-Evaluation-BenchMark/blob/main/evaluate/A3-B1_bench.py).
```
First set the proxy:
os.environ['HTTPS_PROXY']="your proxy"
Nextï¼Œfill your OpenAI Keyï¼š
openai_api_key = "your key"
Run withï¼š
python A12_bench.py
python A3-B1_bench.py
```

Finallyï¼ŒYou can run with [correct_analyse.py](https://github.com/ywjawmw/ShenNong-TCM-Evaluation-BenchMark/blob/main/evaluate/correct_analyse.py) to get the accrucyã€‚
 ```
python correct_analyse.py
 ``` 


ğŸ‘¨â€âš•ï¸ Welcome everyone to follow our open source project for TCM LLM **ShenNong-TCM**, this is the first versionï¼š

- ğŸš€ [ShenNong-TCM](https://github.com/michael-wzhu/ShenNong-TCM-LLM) : To promote the development and implementation of LLM in the field of TCM, enhance LLM's knowledge and ability to answer medical consultations in the field of traditional Chinese medicine, we have launched the **ShenNong** Large scale Language Model for Traditional Chinese Medicine. Based on the [TCM prompt dataset: ShenNong_TCM_Dataset](https://huggingface.co/datasets/michaelwzhu/ShenNong_TCM_Dataset)ã€‚

And our other open source projects for medical LLMsï¼š
- ğŸš€ [Intelligent TCM Inheritance and Innovation Assistance Platform](https://github.com/ywjawmw/AI4TCM-Platform) ;
- ğŸš€ [ChatMed-Consult](https://huggingface.co/michaelwzhu/ChatMed-Consult) ï¼›
- ğŸš€ [PromptCBLUE](https://github.com/michael-wzhu/PromptCBLUE);

## Acknowledge

- [ChatGPT](https://openai.com/blog/chatgpt)
- [ChatGLM](https://github.com/THUDM/ChatGLM-6B)
- [GaoKao-Bench](https://github.com/OpenLMLab/GAOKAO-Bench)


## Citation

Please citeï¼š


## Team Introduction

This project was completed by the Intelligent Knowledge Management and Service Team of the School of Computer Science and Technology, East China Normal University, under the guidance of Professor Xiaoling Wang .

Project membersï¼š
- [ywjawmw](https://github.com/ywjawmw)
- [michael-wzhu](https://github.com/michael-wzhu)
